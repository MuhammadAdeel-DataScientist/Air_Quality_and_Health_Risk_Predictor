{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "081c068a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE ENGINEERING - AIR QUALITY PREDICTION\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering for Air Quality Prediction\n",
    "# This notebook creates advanced features for time-series forecasting\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE ENGINEERING - AIR QUALITY PREDICTION\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f8f6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ LOADING DATA...\n",
      "----------------------------------------------------------------------\n",
      "‚úì Loaded 22,556 records\n",
      "  Date range: 2025-08-31 18:00:00 to 2025-12-03 16:00:00\n",
      "  Columns: 31\n",
      "‚úì Data sorted by city and timestamp\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. LOAD PROCESSED DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìÇ LOADING DATA...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Load corrected data with temporal features\n",
    "data_path = Path('../data/processed/air_quality_with_features.csv')\n",
    "\n",
    "if not data_path.exists():\n",
    "    # Fallback to corrected historical data\n",
    "    data_path = Path('../data/processed/corrected_air_quality_historical_20251129.csv')\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "print(f\"‚úì Loaded {len(df):,} records\")\n",
    "print(f\"  Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "\n",
    "# Sort by city and timestamp for proper lag calculations\n",
    "df = df.sort_values(['city_name', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "print(\"‚úì Data sorted by city and timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5477d24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîô CREATING LAG FEATURES\n",
      "======================================================================\n",
      "Creating lag features for: ['aqi', 'pm25', 'pm10', 'temperature', 'humidity', 'pressure', 'wind_speed']\n",
      "Lag periods: [1, 3, 6, 12, 24, 48, 72, 168] hours\n",
      "\n",
      "‚úì Created 56 lag features\n",
      "‚úì Records with complete lag data: 20,876 (92.6%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. CREATE LAG FEATURES (Historical Values)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üîô CREATING LAG FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define lag periods (in hours)\n",
    "lag_hours = [1, 3, 6, 12, 24, 48, 72, 168]  # 1h to 1 week\n",
    "\n",
    "# Features to create lags for\n",
    "lag_features = ['aqi', 'pm25', 'pm10', 'temperature', 'humidity', \n",
    "                'pressure', 'wind_speed']\n",
    "\n",
    "lag_features_available = [f for f in lag_features if f in df.columns]\n",
    "\n",
    "print(f\"Creating lag features for: {lag_features_available}\")\n",
    "print(f\"Lag periods: {lag_hours} hours\")\n",
    "\n",
    "# Create lag features by city (to avoid cross-city contamination)\n",
    "for feature in lag_features_available:\n",
    "    for lag in lag_hours:\n",
    "        col_name = f'{feature}_lag_{lag}h'\n",
    "        df[col_name] = df.groupby('city_name')[feature].shift(lag)\n",
    "        \n",
    "print(f\"\\n‚úì Created {len(lag_features_available) * len(lag_hours)} lag features\")\n",
    "\n",
    "# Check how many records have complete lag data\n",
    "complete_mask = df[[f'{f}_lag_{lag_hours[-1]}h' for f in lag_features_available]].notna().all(axis=1)\n",
    "print(f\"‚úì Records with complete lag data: {complete_mask.sum():,} ({complete_mask.sum()/len(df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "942156e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä CREATING ROLLING STATISTICS\n",
      "======================================================================\n",
      "Creating rolling statistics for: ['aqi', 'pm25', 'pm10', 'temperature', 'humidity']\n",
      "Windows: [3, 6, 12, 24, 48, 72] hours\n",
      "Statistics: ['mean', 'std', 'min', 'max']\n",
      "\n",
      "‚úì Created 120 rolling statistics features\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. CREATE ROLLING STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä CREATING ROLLING STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define rolling windows (in hours)\n",
    "windows = [3, 6, 12, 24, 48, 72]\n",
    "\n",
    "# Statistics to calculate\n",
    "stats = {\n",
    "    'mean': 'mean',\n",
    "    'std': 'std',\n",
    "    'min': 'min',\n",
    "    'max': 'max'\n",
    "}\n",
    "\n",
    "# Features for rolling stats\n",
    "rolling_features = ['aqi', 'pm25', 'pm10', 'temperature', 'humidity']\n",
    "rolling_features_available = [f for f in rolling_features if f in df.columns]\n",
    "\n",
    "print(f\"Creating rolling statistics for: {rolling_features_available}\")\n",
    "print(f\"Windows: {windows} hours\")\n",
    "print(f\"Statistics: {list(stats.keys())}\")\n",
    "\n",
    "feature_count = 0\n",
    "for feature in rolling_features_available:\n",
    "    for window in windows:\n",
    "        for stat_name, stat_func in stats.items():\n",
    "            col_name = f'{feature}_rolling_{window}h_{stat_name}'\n",
    "            df[col_name] = df.groupby('city_name')[feature].transform(\n",
    "                lambda x: x.rolling(window=window, min_periods=1).agg(stat_func)\n",
    "            )\n",
    "            feature_count += 1\n",
    "\n",
    "print(f\"\\n‚úì Created {feature_count} rolling statistics features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df278468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìà CREATING RATE OF CHANGE FEATURES\n",
      "======================================================================\n",
      "Creating rate of change for: ['aqi', 'pm25', 'temperature', 'pressure']\n",
      "Periods: [1, 3, 6, 12, 24] hours\n",
      "\n",
      "‚úì Created 40 rate of change features\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. CREATE RATE OF CHANGE FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìà CREATING RATE OF CHANGE FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Time periods for rate of change\n",
    "change_periods = [1, 3, 6, 12, 24]\n",
    "\n",
    "change_features = ['aqi', 'pm25', 'temperature', 'pressure']\n",
    "change_features_available = [f for f in change_features if f in df.columns]\n",
    "\n",
    "print(f\"Creating rate of change for: {change_features_available}\")\n",
    "print(f\"Periods: {change_periods} hours\")\n",
    "\n",
    "for feature in change_features_available:\n",
    "    for period in change_periods:\n",
    "        # Absolute change\n",
    "        col_name = f'{feature}_change_{period}h'\n",
    "        df[col_name] = df.groupby('city_name')[feature].diff(period)\n",
    "        \n",
    "        # Percentage change\n",
    "        col_name_pct = f'{feature}_pct_change_{period}h'\n",
    "        df[col_name_pct] = df.groupby('city_name')[feature].pct_change(period) * 100\n",
    "\n",
    "print(f\"\\n‚úì Created {len(change_features_available) * len(change_periods) * 2} rate of change features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78eb49ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚è∞ CREATING CYCLICAL TEMPORAL FEATURES\n",
      "======================================================================\n",
      "‚úì Created cyclical encodings:\n",
      "  - Hour (sin/cos)\n",
      "  - Day of week (sin/cos)\n",
      "  - Month (sin/cos)\n",
      "  - Day of year (sin/cos)\n",
      "‚úì Created categorical temporal features:\n",
      "  - is_weekend, is_rush_hour, is_night, is_peak_pollution\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. CREATE TEMPORAL FEATURES (CYCLICAL ENCODING)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚è∞ CREATING CYCLICAL TEMPORAL FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract temporal components if not already present\n",
    "if 'hour' not in df.columns:\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "if 'day_of_week' not in df.columns:\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "if 'month' not in df.columns:\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "if 'day_of_year' not in df.columns:\n",
    "    df['day_of_year'] = df['timestamp'].dt.dayofyear\n",
    "\n",
    "# Cyclical encoding for hour (0-23)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "# Cyclical encoding for day of week (0-6)\n",
    "df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "# Cyclical encoding for month (1-12)\n",
    "df['month_sin'] = np.sin(2 * np.pi * (df['month'] - 1) / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * (df['month'] - 1) / 12)\n",
    "\n",
    "# Cyclical encoding for day of year (1-365)\n",
    "df['day_of_year_sin'] = np.sin(2 * np.pi * (df['day_of_year'] - 1) / 365)\n",
    "df['day_of_year_cos'] = np.cos(2 * np.pi * (df['day_of_year'] - 1) / 365)\n",
    "\n",
    "print(\"‚úì Created cyclical encodings:\")\n",
    "print(\"  - Hour (sin/cos)\")\n",
    "print(\"  - Day of week (sin/cos)\")\n",
    "print(\"  - Month (sin/cos)\")\n",
    "print(\"  - Day of year (sin/cos)\")\n",
    "\n",
    "# Additional temporal features\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_rush_hour'] = df['hour'].isin([7, 8, 9, 17, 18, 19, 20]).astype(int)\n",
    "df['is_night'] = df['hour'].isin(range(0, 6)).astype(int)\n",
    "df['is_peak_pollution'] = df['hour'].isin([19, 20, 21]).astype(int)\n",
    "\n",
    "print(\"‚úì Created categorical temporal features:\")\n",
    "print(\"  - is_weekend, is_rush_hour, is_night, is_peak_pollution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0da2b301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üå§Ô∏è CREATING WEATHER INTERACTION FEATURES\n",
      "======================================================================\n",
      "‚úì Created: temp_humidity_interaction\n",
      "‚úì Created: temperature_squared\n",
      "‚úì Created: humidity_squared\n",
      "‚úì Created: wind_chill\n",
      "‚úì Created: pressure_change_3h\n",
      "‚úì Created: comfort_index\n",
      "\n",
      "‚úì Created weather interaction features\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. CREATE WEATHER INTERACTION FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üå§Ô∏è CREATING WEATHER INTERACTION FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "weather_features = ['temperature', 'humidity', 'pressure', 'wind_speed']\n",
    "available_weather = [f for f in weather_features if f in df.columns and df[f].notna().sum() > 1000]\n",
    "\n",
    "if len(available_weather) >= 2:\n",
    "    # Temperature √ó Humidity\n",
    "    if 'temperature' in available_weather and 'humidity' in available_weather:\n",
    "        df['temp_humidity_interaction'] = df['temperature'] * df['humidity']\n",
    "        print(\"‚úì Created: temp_humidity_interaction\")\n",
    "    \n",
    "    # Temperature squared (non-linear effects)\n",
    "    if 'temperature' in available_weather:\n",
    "        df['temperature_squared'] = df['temperature'] ** 2\n",
    "        print(\"‚úì Created: temperature_squared\")\n",
    "    \n",
    "    # Humidity squared\n",
    "    if 'humidity' in available_weather:\n",
    "        df['humidity_squared'] = df['humidity'] ** 2\n",
    "        print(\"‚úì Created: humidity_squared\")\n",
    "    \n",
    "    # Wind chill approximation\n",
    "    if 'temperature' in available_weather and 'wind_speed' in available_weather:\n",
    "        df['wind_chill'] = df['temperature'] - (df['wind_speed'] * 0.5)\n",
    "        print(\"‚úì Created: wind_chill\")\n",
    "    \n",
    "    # Pressure change (indicates weather fronts)\n",
    "    if 'pressure' in available_weather:\n",
    "        df['pressure_change_3h'] = df.groupby('city_name')['pressure'].diff(3)\n",
    "        print(\"‚úì Created: pressure_change_3h\")\n",
    "    \n",
    "    # Comfort index (temperature + humidity)\n",
    "    if 'temperature' in available_weather and 'humidity' in available_weather:\n",
    "        df['comfort_index'] = df['temperature'] + (0.4 * df['humidity'])\n",
    "        print(\"‚úì Created: comfort_index\")\n",
    "\n",
    "print(f\"\\n‚úì Created weather interaction features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f35e8fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üè≠ CREATING POLLUTANT RATIO FEATURES\n",
      "======================================================================\n",
      "‚úì Created: pm25_pm10_ratio\n",
      "‚úì Created: no2_o3_ratio\n",
      "‚úì Created: total_pm\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. CREATE POLLUTANT RATIOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üè≠ CREATING POLLUTANT RATIO FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# PM2.5 to PM10 ratio (indicator of fine vs coarse particles)\n",
    "if 'pm25' in df.columns and 'pm10' in df.columns:\n",
    "    df['pm25_pm10_ratio'] = df['pm25'] / (df['pm10'] + 1e-6)  # Avoid division by zero\n",
    "    print(\"‚úì Created: pm25_pm10_ratio\")\n",
    "\n",
    "# NO2 to O3 ratio (indicator of photochemical activity)\n",
    "if 'no2' in df.columns and 'o3' in df.columns:\n",
    "    df['no2_o3_ratio'] = df['no2'] / (df['o3'] + 1e-6)\n",
    "    print(\"‚úì Created: no2_o3_ratio\")\n",
    "\n",
    "# Total particulate matter\n",
    "if 'pm25' in df.columns and 'pm10' in df.columns:\n",
    "    df['total_pm'] = df['pm25'] + df['pm10']\n",
    "    print(\"‚úì Created: total_pm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762043c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üè∑Ô∏è ENCODING CATEGORICAL FEATURES\n",
      "======================================================================\n",
      "‚úì Encoded city_name: 10 unique cities\n",
      "  City encoding: {'Beijing': np.int64(0), 'Cairo': np.int64(1), 'Delhi': np.int64(2), 'London': np.int64(3), 'Los Angeles': np.int64(4), 'Mexico City': np.int64(5), 'Mumbai': np.int64(6), 'New York': np.int64(7), 'S√£o Paulo': np.int64(8), 'Tokyo': np.int64(9)}\n",
      "‚úì Encoded country_code: 8 unique countries\n",
      "‚úì One-hot encoded aqi_category: 6 categories\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 8. ENCODE CATEGORICAL FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üè∑Ô∏è ENCODING CATEGORICAL FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Label encode city names\n",
    "if 'city_name' in df.columns:\n",
    "    le_city = LabelEncoder()\n",
    "    df['city_encoded'] = le_city.fit_transform(df['city_name'])\n",
    "    print(f\"‚úì Encoded city_name: {len(le_city.classes_)} unique cities\")\n",
    "    \n",
    "    # Save mapping\n",
    "    city_mapping = dict(zip(le_city.classes_, le_city.transform(le_city.classes_)))\n",
    "    print(\"  City encoding:\", city_mapping)\n",
    "\n",
    "# Label encode country\n",
    "if 'country_code' in df.columns:\n",
    "    le_country = LabelEncoder()\n",
    "    df['country_encoded'] = le_country.fit_transform(df['country_code'])\n",
    "    print(f\"‚úì Encoded country_code: {len(le_country.classes_)} unique countries\")\n",
    "\n",
    "# One-hot encode AQI category (if exists)\n",
    "if 'aqi_category' in df.columns:\n",
    "    df_category_dummies = pd.get_dummies(df['aqi_category'], prefix='aqi_cat')\n",
    "    df = pd.concat([df, df_category_dummies], axis=1)\n",
    "    print(f\"‚úì One-hot encoded aqi_category: {len(df_category_dummies.columns)} categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9118e36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìã FEATURE ENGINEERING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Feature counts by type:\n",
      "  Original Features.............   39\n",
      "  Lag Features..................   56\n",
      "  Rolling Statistics............  120\n",
      "  Rate of Change................   40\n",
      "  Cyclical Temporal.............    8\n",
      "  Weather Interactions..........    5\n",
      "  Pollutant Ratios..............    3\n",
      "  Encoded Features..............    8\n",
      "\n",
      "Total columns:                  275\n",
      "Total records:                  22,556\n",
      "\n",
      "‚ö†Ô∏è  Columns with >50% missing data: 4\n",
      "   (These will be dropped)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 9. FEATURE SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìã FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Count features by type\n",
    "feature_types = {\n",
    "    'Original Features': len([c for c in df.columns if not any(x in c for x in ['lag', 'rolling', 'change', 'sin', 'cos', 'interaction', 'ratio', 'encoded', 'aqi_cat'])]),\n",
    "    'Lag Features': len([c for c in df.columns if 'lag' in c]),\n",
    "    'Rolling Statistics': len([c for c in df.columns if 'rolling' in c]),\n",
    "    'Rate of Change': len([c for c in df.columns if 'change' in c]),\n",
    "    'Cyclical Temporal': len([c for c in df.columns if 'sin' in c or 'cos' in c]),\n",
    "    'Weather Interactions': len([c for c in df.columns if 'interaction' in c or 'squared' in c or 'chill' in c or 'comfort' in c]),\n",
    "    'Pollutant Ratios': len([c for c in df.columns if 'ratio' in c or 'total_pm' in c]),\n",
    "    'Encoded Features': len([c for c in df.columns if 'encoded' in c or 'aqi_cat_' in c])\n",
    "}\n",
    "\n",
    "print(\"\\nFeature counts by type:\")\n",
    "for ftype, count in feature_types.items():\n",
    "    print(f\"  {ftype:.<30} {count:>4}\")\n",
    "\n",
    "print(f\"\\n{'Total columns:':<30} {len(df.columns):>4}\")\n",
    "print(f\"{'Total records:':<30} {len(df):>7,}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_summary = df.isnull().sum()\n",
    "missing_pct = (missing_summary / len(df) * 100).round(2)\n",
    "high_missing = missing_summary[missing_pct > 50]\n",
    "\n",
    "if len(high_missing) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Columns with >50% missing data: {len(high_missing)}\")\n",
    "    print(\"   (These will be dropped)\")\n",
    "else:\n",
    "    print(\"\\n‚úì No columns with excessive missing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81f6bc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä PRELIMINARY FEATURE IMPORTANCE\n",
      "======================================================================\n",
      "Using 0 complete records for importance analysis\n",
      "Analyzing 251 features\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 10. VISUALIZE FEATURE IMPORTANCE (PRELIMINARY)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä PRELIMINARY FEATURE IMPORTANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Select numeric features only\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove target and non-predictive features\n",
    "exclude_features = ['aqi', 'timestamp', 'date', 'latitude', 'longitude', \n",
    "                   'hour', 'day_of_week', 'month', 'day_of_year']\n",
    "feature_cols = [c for c in numeric_features if c not in exclude_features and not c.startswith('aqi_cat_')]\n",
    "\n",
    "# Use complete cases for preliminary analysis\n",
    "df_complete = df[feature_cols + ['aqi']].dropna()\n",
    "\n",
    "print(f\"Using {len(df_complete):,} complete records for importance analysis\")\n",
    "print(f\"Analyzing {len(feature_cols)} features\")\n",
    "\n",
    "if len(df_complete) > 100 and len(feature_cols) > 0:\n",
    "    # Sample if dataset is too large\n",
    "    if len(df_complete) > 10000:\n",
    "        df_sample = df_complete.sample(10000, random_state=42)\n",
    "    else:\n",
    "        df_sample = df_complete\n",
    "    \n",
    "    # Calculate mutual information\n",
    "    X = df_sample[feature_cols]\n",
    "    y = df_sample['aqi']\n",
    "    \n",
    "    mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "    mi_scores = pd.Series(mi_scores, index=feature_cols).sort_values(ascending=False)\n",
    "    \n",
    "    # Plot top 20\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    mi_scores.head(20).sort_values().plot(kind='barh', ax=ax, color='steelblue')\n",
    "    ax.set_title('Top 20 Features by Mutual Information', fontweight='bold', fontsize=14)\n",
    "    ax.set_xlabel('Mutual Information Score')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úì Top 10 most important features:\")\n",
    "    for i, (feature, score) in enumerate(mi_scores.head(10).items(), 1):\n",
    "        print(f\"  {i:2d}. {feature:<40} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9c721c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîß HANDLING MISSING VALUES\n",
      "======================================================================\n",
      "Dropping 4 columns with >80.0% missing:\n",
      "  - location\n",
      "  - city\n",
      "  - country\n",
      "  - state\n",
      "\n",
      "‚úì Filled lag/rolling features within cities\n",
      "‚úì Filled weather features with city medians\n",
      "\n",
      "Remaining missing values: 12,040 (0.20%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 11. HANDLE MISSING VALUES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üîß HANDLING MISSING VALUES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Drop columns with >80% missing\n",
    "threshold = 0.8\n",
    "missing_pct = df.isnull().sum() / len(df)\n",
    "cols_to_drop = missing_pct[missing_pct > threshold].index.tolist()\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"Dropping {len(cols_to_drop)} columns with >{threshold*100}% missing:\")\n",
    "    for col in cols_to_drop[:10]:  # Show first 10\n",
    "        print(f\"  - {col}\")\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# For lag features, forward fill within each city (up to 3 periods)\n",
    "lag_cols = [c for c in df.columns if 'lag' in c or 'rolling' in c]\n",
    "for col in lag_cols:\n",
    "    df[col] = df.groupby('city_name')[col].fillna(method='ffill', limit=3)\n",
    "\n",
    "print(f\"\\n‚úì Filled lag/rolling features within cities\")\n",
    "\n",
    "# Fill weather features with city-specific medians\n",
    "weather_cols = [c for c in df.columns if any(w in c for w in ['temperature', 'humidity', 'pressure', 'wind'])]\n",
    "for col in weather_cols:\n",
    "    if col in df.columns and df[col].isnull().sum() > 0:\n",
    "        df[col] = df.groupby('city_name')[col].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "print(f\"‚úì Filled weather features with city medians\")\n",
    "\n",
    "# Check remaining missing\n",
    "remaining_missing = df.isnull().sum().sum()\n",
    "print(f\"\\nRemaining missing values: {remaining_missing:,} ({remaining_missing/(len(df)*len(df.columns))*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f7c7ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÇÔ∏è CREATING TRAIN/VAL/TEST SPLITS\n",
      "======================================================================\n",
      "Train set: 15,789 records (70.0%)\n",
      "  Date range: 2025-08-31 18:00:00 to 2025-11-05 15:00:00\n",
      "\n",
      "Validation set: 3,383 records (15.0%)\n",
      "  Date range: 2025-11-05 15:00:00 to 2025-11-19 17:00:00\n",
      "\n",
      "Test set: 3,384 records (15.0%)\n",
      "  Date range: 2025-11-19 17:00:00 to 2025-12-03 16:00:00\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 12. CREATE TRAIN/VALIDATION/TEST SPLITS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÇÔ∏è CREATING TRAIN/VAL/TEST SPLITS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Sort by timestamp\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# Use temporal split (important for time series)\n",
    "train_size = int(0.7 * len(df))\n",
    "val_size = int(0.15 * len(df))\n",
    "\n",
    "df_train = df.iloc[:train_size].copy()\n",
    "df_val = df.iloc[train_size:train_size+val_size].copy()\n",
    "df_test = df.iloc[train_size+val_size:].copy()\n",
    "\n",
    "print(f\"Train set: {len(df_train):,} records ({len(df_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Date range: {df_train['timestamp'].min()} to {df_train['timestamp'].max()}\")\n",
    "\n",
    "print(f\"\\nValidation set: {len(df_val):,} records ({len(df_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Date range: {df_val['timestamp'].min()} to {df_val['timestamp'].max()}\")\n",
    "\n",
    "print(f\"\\nTest set: {len(df_test):,} records ({len(df_test)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Date range: {df_test['timestamp'].min()} to {df_test['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfe120bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üíæ SAVING ENGINEERED FEATURES\n",
      "======================================================================\n",
      "‚úì Saved full dataset: ..\\data\\processed\\features_engineered_full.csv\n",
      "‚úì Saved train set: ..\\data\\processed\\features_train.csv\n",
      "‚úì Saved validation set: ..\\data\\processed\\features_val.csv\n",
      "‚úì Saved test set: ..\\data\\processed\\features_test.csv\n",
      "‚úì Saved feature list: ..\\data\\processed\\feature_list.json\n",
      "\n",
      "======================================================================\n",
      "‚úÖ FEATURE ENGINEERING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "üìä Final Dataset Statistics:\n",
      "  Total records: 22,556\n",
      "  Total features: 271\n",
      "  Train records: 15,789\n",
      "  Val records: 3,383\n",
      "  Test records: 3,384\n",
      "  Missing data: 0.20%\n",
      "\n",
      "üéØ Next Steps:\n",
      "1. Review feature importance analysis\n",
      "2. Proceed to model training (Phase 5)\n",
      "3. Use features_train.csv, features_val.csv, features_test.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 13. SAVE ENGINEERED FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üíæ SAVING ENGINEERED FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save full dataset\n",
    "full_path = output_dir / 'features_engineered_full.csv'\n",
    "df.to_csv(full_path, index=False)\n",
    "print(f\"‚úì Saved full dataset: {full_path}\")\n",
    "\n",
    "# Save train/val/test sets\n",
    "train_path = output_dir / 'features_train.csv'\n",
    "val_path = output_dir / 'features_val.csv'\n",
    "test_path = output_dir / 'features_test.csv'\n",
    "\n",
    "df_train.to_csv(train_path, index=False)\n",
    "df_val.to_csv(val_path, index=False)\n",
    "df_test.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"‚úì Saved train set: {train_path}\")\n",
    "print(f\"‚úì Saved validation set: {val_path}\")\n",
    "print(f\"‚úì Saved test set: {test_path}\")\n",
    "\n",
    "# Save feature list\n",
    "feature_list = {\n",
    "    'all_features': df.columns.tolist(),\n",
    "    'numeric_features': df.select_dtypes(include=[np.number]).columns.tolist(),\n",
    "    'categorical_features': df.select_dtypes(include=['object']).columns.tolist(),\n",
    "    'lag_features': [c for c in df.columns if 'lag' in c],\n",
    "    'rolling_features': [c for c in df.columns if 'rolling' in c],\n",
    "    'temporal_features': [c for c in df.columns if any(t in c for t in ['sin', 'cos', 'hour', 'dow', 'month', 'weekend', 'rush', 'night'])]\n",
    "}\n",
    "\n",
    "import json\n",
    "feature_list_path = output_dir / 'feature_list.json'\n",
    "with open(feature_list_path, 'w') as f:\n",
    "    json.dump(feature_list, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Saved feature list: {feature_list_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä Final Dataset Statistics:\")\n",
    "print(f\"  Total records: {len(df):,}\")\n",
    "print(f\"  Total features: {len(df.columns)}\")\n",
    "print(f\"  Train records: {len(df_train):,}\")\n",
    "print(f\"  Val records: {len(df_val):,}\")\n",
    "print(f\"  Test records: {len(df_test):,}\")\n",
    "print(f\"  Missing data: {df.isnull().sum().sum()/(len(df)*len(df.columns))*100:.2f}%\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"1. Review feature importance analysis\")\n",
    "print(\"2. Proceed to model training (Phase 5)\")\n",
    "print(\"3. Use features_train.csv, features_val.csv, features_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
